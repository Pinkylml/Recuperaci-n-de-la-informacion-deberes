{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab838f6833b5dfdb",
   "metadata": {},
   "source": [
    "# Integrating Embeddings with Queries in an Information Retrieval System\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this exercise, we will learn how to integrate embeddings with a query to enhance an Information Retrieval (IR) system. We will use both static and contextual embeddings to generate representations of queries and documents, compute their similarities, and rank the documents based on relevance to the query.\n",
    "\n",
    "---\n",
    "\n",
    "## Stages Covered\n",
    "\n",
    "1. **Introduction to Pre-trained Transformer Models**\n",
    "   - Load and use BERT for contextual embeddings.\n",
    "   - Load and use Word2Vec for static embeddings.\n",
    "\n",
    "2. **Generating Text Embeddings**\n",
    "   - Generate embeddings for queries and documents using BERT.\n",
    "   - Generate embeddings for queries and documents using Word2Vec.\n",
    "\n",
    "3. **Computing Similarity Between Embeddings**\n",
    "   - Compute cosine similarity between query and document embeddings.\n",
    "   - Rank documents based on similarity scores.\n",
    "\n",
    "4. **Integrating Embeddings with Queries**\n",
    "   - Practical implementation of embedding-based retrieval for a given text corpus.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- TensorFlow\n",
    "- Hugging Face's Transformers library\n",
    "- Gensim library\n",
    "- Scikit-learn library\n",
    "- A text corpus in the `../data` folder\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Follow the steps below to integrate embeddings with a query and enhance your IR system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96332a5cf057238",
   "metadata": {},
   "source": [
    "Step 0: Verify requirements:\n",
    "\n",
    "* tensorflow\n",
    "* transformers\n",
    "* scikit-learn\n",
    "* matplotlib\n",
    "* seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f333a9f10d93ad",
   "metadata": {},
   "source": [
    "Step 1: Download dataset from Kaggle\n",
    "\n",
    "URL: https://www.kaggle.com/datasets/zynicide/wine-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bab6504ab1e044a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T21:05:14.438938Z",
     "start_time": "2024-06-26T21:05:12.777794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/jeffersonc/.kaggle/kaggle.json'\n",
      "Dataset URL: https://www.kaggle.com/datasets/zynicide/wine-reviews\n",
      "License(s): CC-BY-NC-SA-4.0\n",
      "wine-reviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "import pandas as pd\n",
    "\n",
    "kaggle.api.dataset_download_cli(dataset='zynicide/wine-reviews')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39684bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!unzip wine-reviews.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c78933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 country                                        description  \\\n",
      "0           0      US  This tremendous 100% varietal wine hails from ...   \n",
      "1           1   Spain  Ripe aromas of fig, blackberry and cassis are ...   \n",
      "2           2      US  Mac Watson honors the memory of a wine once ma...   \n",
      "3           3      US  This spent 20 months in 30% new French oak, an...   \n",
      "4           4  France  This is the top wine from La Bégude, named aft...   \n",
      "\n",
      "                            designation  points  price        province  \\\n",
      "0                     Martha's Vineyard      96  235.0      California   \n",
      "1  Carodorum Selección Especial Reserva      96  110.0  Northern Spain   \n",
      "2         Special Selected Late Harvest      96   90.0      California   \n",
      "3                               Reserve      96   65.0          Oregon   \n",
      "4                            La Brûlade      95   66.0        Provence   \n",
      "\n",
      "            region_1           region_2             variety  \\\n",
      "0        Napa Valley               Napa  Cabernet Sauvignon   \n",
      "1               Toro                NaN       Tinta de Toro   \n",
      "2     Knights Valley             Sonoma     Sauvignon Blanc   \n",
      "3  Willamette Valley  Willamette Valley          Pinot Noir   \n",
      "4             Bandol                NaN  Provence red blend   \n",
      "\n",
      "                    winery  \n",
      "0                    Heitz  \n",
      "1  Bodega Carmen Rodríguez  \n",
      "2                 Macauley  \n",
      "3                    Ponzi  \n",
      "4     Domaine de la Bégude  \n"
     ]
    }
   ],
   "source": [
    "wine_df = pd.read_csv('data/winemag-data_first150k.csv')\n",
    "print(wine_df.head())\n",
    "corpus = wine_df['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef79a9c697bc01",
   "metadata": {},
   "source": [
    "Step 2: Load a Pre-trained Transformer Model\n",
    "\n",
    "Use the BERT model for generating contextual embeddings and Word2Vec for static embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad22ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-06 17:00:17.103256: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-06 17:00:22.225213: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-06 17:00:25.080634: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-06 17:00:26.521552: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-06 17:00:26.526654: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-06 17:00:28.233874: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-06 17:00:35.315525: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "427e86e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68840411ddabd35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T21:06:28.907164Z",
     "start_time": "2024-06-26T21:05:21.570520Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffersonc/Documentos/RecuperacionInformacion/envir/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2277791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec model\n",
    "word2vec_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ce2c32618191",
   "metadata": {},
   "source": [
    "Step 3: Generate Text Embeddings\n",
    "\n",
    "Static Embeddings with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53983765626d5a85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T21:07:16.011459Z",
     "start_time": "2024-06-26T21:07:15.967338Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_word2vec_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        tokens = text.lower().split()\n",
    "        word_vectors = [word2vec_model[word] for word in tokens if word in word2vec_model]\n",
    "        if word_vectors:\n",
    "            embeddings.append(np.mean(word_vectors, axis=0))\n",
    "        else:\n",
    "            embeddings.append(np.zeros(word2vec_model.vector_size))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "word2vec_embeddings = generate_word2vec_embeddings(corpus[:10])\n",
    "print(\"Word2Vec Embeddings:\", word2vec_embeddings)\n",
    "print(\"Word2Vec Shape:\", word2vec_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2eee6a2cae63f9",
   "metadata": {},
   "source": [
    "Contextual Embeddings with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deddc992f933fcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T02:43:37.685104Z",
     "start_time": "2024-06-27T02:43:33.304379Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_bert_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True)\n",
    "        outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state[:, 0, :])  # Use [CLS] token representation\n",
    "    return np.array(embeddings).transpose(0,2,1)\n",
    "\n",
    "bert_embeddings = generate_bert_embeddings(corpus[:10])\n",
    "print(\"BERT Embeddings:\", bert_embeddings)\n",
    "print(\"Word2Vec Shape:\", bert_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f905d2f5fe852",
   "metadata": {},
   "source": [
    "Step 4: Compute Similarity Between Embeddings\n",
    "\n",
    "Use the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c1a78ceff1d324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T02:46:20.622551Z",
     "start_time": "2024-06-27T02:46:20.606815Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Cosine similarity between Word2Vec embeddings\n",
    "word2vec_similarity = cosine_similarity(word2vec_embeddings)\n",
    "print(\"Word2Vec Cosine Similarity:\\n\", word2vec_similarity)\n",
    "\n",
    "# Cosine similarity between BERT embeddings\n",
    "bert_similarity = cosine_similarity(bert_embeddings.reshape(10,768))\n",
    "print(\"BERT Cosine Similarity:\\n\", bert_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46c4c975cb5c7b",
   "metadata": {},
   "source": [
    "Step 5: Compare Contextual and Static Embeddings\n",
    "\n",
    "Analyze and compare the similarity results from both BERT and Word2Vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416e3d9334cd21e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:25:28.401064Z",
     "start_time": "2024-06-27T03:25:27.726620Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_similarity_matrix(matrix, title, figsize=(8, 6), annotation=True):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(matrix, annot=annotation, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_similarity_matrix(word2vec_similarity, \"Word2Vec Cosine Similarity\")\n",
    "plot_similarity_matrix(bert_similarity, \"BERT Cosine Similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1bd3d58f27a375",
   "metadata": {},
   "source": [
    "Step 6: Applying to Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696a6b93f33889b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:27:06.030478Z",
     "start_time": "2024-06-27T03:25:33.511959Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for the corpus\n",
    "corpus_word2vec_embeddings = generate_word2vec_embeddings(corpus[:500])\n",
    "corpus_bert_embeddings = generate_bert_embeddings(corpus[:500])\n",
    "\n",
    "# Compute similarity for the corpus\n",
    "corpus_word2vec_similarity = cosine_similarity(corpus_word2vec_embeddings)\n",
    "corpus_bert_similarity = cosine_similarity(corpus_bert_embeddings.reshape(corpus_bert_embeddings.shape[:2]))\n",
    "\n",
    "# Plot similarity matrices\n",
    "plot_similarity_matrix(corpus_word2vec_similarity, \"Corpus Word2Vec Cosine Similarity\", figsize=(16, 12), annotation=False)\n",
    "plot_similarity_matrix(corpus_bert_similarity, \"Corpus BERT Cosine Similarity\", figsize=(16, 12), annotation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bac5300222c91be",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "So far, in this exercise, you learned how to:\n",
    "\n",
    "* Load a pre-trained transformer model (BERT) and a static embedding model (Word2Vec).\n",
    "* Generate text embeddings using these models.\n",
    "* Compute cosine similarity between embeddings.\n",
    "* Compare the similarity results from both contextual and static embeddings.\n",
    "\n",
    "Now you have a practical understanding of how transformers and embeddings can be used in Information Retrieval systems.\n",
    "\n",
    "Let's integrate query search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc424507979ba874",
   "metadata": {},
   "source": [
    "Step 7: Generate Embeddings for the Query\n",
    "\n",
    "Generate embeddings for the query using the same model used for the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658207660a566a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "389cb8f527843221",
   "metadata": {},
   "source": [
    "Step 8: Compute Similarity Between Query and Documents\n",
    "\n",
    "Compute the similarity between the query embedding and each document embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5991a0aeababffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "139fea5d43934c96",
   "metadata": {},
   "source": [
    "Step 9: Retrieve and Rank Documents Based on Similarity Scores\n",
    "\n",
    "Retrieve and rank the documents based on their similarity scores to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a6b13bbb20d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7721ececc688c1be",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
